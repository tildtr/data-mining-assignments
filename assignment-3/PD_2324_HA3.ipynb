{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333368e3-bb42-466c-8823-c45ed71f970f",
   "metadata": {},
   "source": [
    "# Prospecção de Dados (Data Mining) DI/FCUL - HA3\n",
    "\n",
    "## Third Home Assignement (MC/DI/FCUL - 2024)\n",
    "\n",
    "### Fill in the section below\n",
    "\n",
    "### GROUP: `13`\n",
    "\n",
    "* Miguel Landum, 35019 - Hours worked on the project\n",
    "* Niklas Schmitz, 62689 - Hours worked on the project\n",
    "* Pol Parra, 62692 - Hours worked on the project\n",
    "* Til Dietrich, 62928 - Hours worked on the project\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The purpose of this Home Assignment is\n",
    "* Find similar items with Local Sensitivity Hashing\n",
    "* Do Dimensionality Reduction\n",
    "\n",
    "**NOTE 1: Students are not allowed to add more cells to the notebook**\n",
    "\n",
    "**NOTE 2: The notebook must be submited fully executed**\n",
    "\n",
    "**NOTE 3: Name of notebook should be: HA3_GROUP-XX.ipynb (where XX is the group number)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56959b24",
   "metadata": {},
   "source": [
    "**NOTE to run code locally:** add data (data_d3.pickle, data_d4.pickle) to **assignment-3/data/** folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40f24-0da5-4b96-90f3-84259cbd1bc5",
   "metadata": {},
   "source": [
    "## 1. Read the Dataset\n",
    "\n",
    "The dataset correspond to about 99% of the Human Proteome (set of known Human Proteins - about 19,500), coded with specific structural elements. They are presented in a dictionary where the key is the [UniprotID](https://www.uniprot.org/) of the protein and the value is a set of indices of a specific structural characteristic\n",
    "\n",
    "Students can use one of two datasets, that are **not** subsets of each other: \n",
    "* `data_d3.pickle` - smaller set of structural features (2048)\n",
    "* `data_d4.pickle` - much larger set of structural features (20736) **Note:** This dataset has been Zipped to fit into moodle. Students should unzip it before usage \n",
    "\n",
    "Select **one** of the datasets and perform all analyses with it. \n",
    "\n",
    "It may be adviseable the usage of sparse matrices, especially for the `d4` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5ba65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasketch\n",
      "  Downloading datasketch-1.6.4-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/niklas/miniconda3/lib/python3.11/site-packages (from datasketch) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/niklas/miniconda3/lib/python3.11/site-packages (from datasketch) (1.11.4)\n",
      "Downloading datasketch-1.6.4-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: datasketch\n",
      "Successfully installed datasketch-1.6.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa918e91-c325-4c1d-8e8d-93d411035a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A024R1R8\n",
      "{1, 771, 775, 263, 137, 11, 531, 1176, 922, 30, 927, 1185, 673, 675, 423, 296, 1704, 2043, 813, 304, 48, 52, 693, 1080, 1083, 1211, 1085, 1469, 69, 454, 1865, 458, 842, 1609, 1618, 467, 1109, 214, 1237, 473, 90, 603, 1379, 229, 1768, 1513, 1260, 109, 1645, 1007, 112, 1521, 242, 2035, 1652, 1525, 1270, 119, 1017, 123, 508, 1149}\n",
      "62\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "### Your code Here\n",
    "import pickle\n",
    "from itertools import islice\n",
    "\n",
    "data_d3=pickle.load(open(\"data/data_d3.pickle\", \"rb\"))\n",
    "data_d4=pickle.load(open(\"data/data_d4.pickle\", \"rb\"))\n",
    "\n",
    "first_elements = list(islice(data_d3, 5))\n",
    "print(first_elements[0])\n",
    "print(data_d3[first_elements[0]])\n",
    "print(len(data_d3[first_elements[0]]))\n",
    "print(len(data_d3[first_elements[1]])) # => elements in dict have different amount of elements in them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853614f3-1e1b-4481-9784-e29dd0b453cb",
   "metadata": {},
   "source": [
    "## 2. Perform Local Sensitivity Hashing (LSH)\n",
    "\n",
    "* examine the selected dataset in terms of similarities and select a set of LSH parameters able to capture the most similar proteins\n",
    "* Comment your results\n",
    "\n",
    "**BONUS POINTS:** It might be interesting to identify **some** of the candidate pairs in Uniprot, to check if they share some of the same properties (e.g. for [protein P28223](https://www.uniprot.org/uniprotkb/P28223/entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead806d9-8073-4fc0-9668-68ad0b89077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add supporting functions here\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def convert_to_binary_matrix(data):\n",
    "    # Extract all unique values to create the universe of elements\n",
    "    all_values = set()\n",
    "    for values in data.values():\n",
    "        all_values.update(values)\n",
    "\n",
    "    # Create a mapping from values to index\n",
    "    value_to_index = {value: idx for idx, value in enumerate(all_values)}\n",
    "\n",
    "    # Number of unique elements\n",
    "    num_elements = len(all_values)\n",
    "\n",
    "    # Prepare data for csr_matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data_values = []\n",
    "\n",
    "    for key, values in data.items():\n",
    "        for value in values:\n",
    "            rows.append(key)\n",
    "            cols.append(value_to_index[value])\n",
    "            data_values.append(1)  # Presence of the element\n",
    "\n",
    "    # Convert document IDs to numerical indices\n",
    "    keys_to_index = {doc: idx for idx, doc in enumerate(data.keys())}\n",
    "    row_indices = [keys_to_index[doc] for doc in rows]\n",
    "\n",
    "    # Create and return CSR matrix\n",
    "    return csr_matrix((data_values, (row_indices, cols)), shape=(len(data), num_elements)), keys_to_index\n",
    "\n",
    "# Create MinHash signatures from sparse matrix\n",
    "def create_minhash_from_sparse(matrix, num_perm):\n",
    "    minhashes = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        minhash = MinHash(num_perm=num_perm)\n",
    "        # reinitialization of minhash is not a problem as the library ensures that the same hash functions are used as long as num_perm stays the same value\n",
    "        for idx in matrix[i].indices:\n",
    "            minhash.update(idx)\n",
    "        minhashes.append(minhash)\n",
    "    return minhashes\n",
    "\n",
    "def init_minhash_lsh(num_perm, threshold):\n",
    "    return MinHashLSH(threshold=threshold, num_perm=num_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f77105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSH index created and signatures inserted.\n"
     ]
    }
   ],
   "source": [
    "csr_data, data_indices = convert_to_binary_matrix(data_d3)\n",
    "# Number of permutations\n",
    "num_perm = 8\n",
    "\n",
    "# Generate MinHash signatures from sparse matrix\n",
    "minhashes = create_minhash_from_sparse(csr_data, num_perm)\n",
    "\n",
    "# Define threshold\n",
    "threshold = 0.5\n",
    "lsh = init_minhash_lsh(num_perm, threshold)\n",
    "\n",
    "# Populate lsh with calculated MinHash signatures\n",
    "for doc, minhash in zip(data_d3.keys(), minhashes):\n",
    "    lsh.insert(doc, minhash)\n",
    "    \n",
    "print(\"\\nLSH index created and signatures inserted.\")\n",
    "\n",
    "# Example Query: Find similar items to specific item key: P28223 => Bonus question\n",
    "query_key = \"P28223\"\n",
    "query_index = data_indices[query_key]\n",
    "results = lsh.query(minhashes[query_index])\n",
    "print(f\"Items similar to {query_key}: {results}\")\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2fc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO all the stuff that I did here is nice and solves the task but he wants\n",
    "# an implementation where we have more control => use the one from the TP: LSHT\n",
    "# Convert the data to binary can be kept and is definetily necessary\n",
    "# Code can be kept to answer the bonus question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a87b8-4102-43ff-ac7c-e748df7bcd1f",
   "metadata": {},
   "source": [
    "### Your short analysis here\n",
    "\n",
    "Plan:\n",
    "- use sparse matrices!\n",
    "- use MinHashing to calculate the Jaccard Similarity as it is a good approach for large datasets (like d4)\n",
    "- use LSHT instead of LSH as it heavily outperformes LSH for sparse matrices (which is the case for this data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8043c37-3a4d-4c4d-ab3e-2fae63af06fa",
   "metadata": {},
   "source": [
    "## 3. Do dimensionality reduction \n",
    "\n",
    "Use the techniques discussued in class to make an appropriate dimensional reduction of the selected dataset. It is not necesary to be extensive, **it is better to select one approach and do it well than try a lot of techniques with poor insights and analysis**\n",
    "\n",
    "It is important to do some sensitivity analysis, relating the dataset size reduction to the loss of information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedab1a-54d8-49a3-8c08-4815bfc7d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add supporting functions here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72095268-f6c8-4156-98ce-662952ba22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add processing code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f4a00-01db-4c2c-a7b7-0a0f7ab0d333",
   "metadata": {},
   "source": [
    "## 4. Discuss your findings [to fill on your own]\n",
    "\n",
    "* Comment your results above\n",
    "* Discuss how could they be used for the full Uniprot that currently has about [248 Million proteins](https://www.uniprot.org/uniprotkb/statistics)\n",
    "\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni-notebooks-3.10",
   "language": "python",
   "name": "uni-notebooks-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
